# 机器学习

## 集成学习

假设有$T$个基分类器， $h_i$表示第i个基分类器， $h_i(x)$表示第i个分类器对于输入$x$的预测结果， $f(x)$表示对于输入$x$的真实结果， 假设每个基分类器的错误率为$\epsilon$, 则有：
>$$  P(h_{i}(x) \neq f(x)) = \epsilon $$

假设超过半数基分类器预测正确则集成分类正确：
>$$ H(x) = sign(\sum_{i=n}^{T} h_i(x)) $$

假设每个分类器的误差相互独立， 根据$\mathcal{Hoeffding}$不等式， 有：
> $$ P(H(x) \neq f(x)) = \sum_{k=0}^{\lfloor T/2 \rfloor}\lgroup {T \atop k} \rgroup (1-\epsilon)^k \epsilon^(T - k)\le exp(- \frac{1}{2} (1-2\epsilon)^2) $$

所以随着基分类器数量的增加， 集成分类器的错误率将降低。

### Boosting

Boosting是一族可以将弱学习器提升为强学习器的算法， 最著名的代表是AdaBoost， 基学习器的线性组合为：
>$$  H(x) = \sum_{t = 1}^{T} \alpha_t h_t (x) $$
优化的损失函数为:
>$$  \mathcal{l}_{exp}(H | \Phi) = \mathbb{E}_{x \sim \Phi}[e^{-f(x)H(x)}] $$
>$$ \frac{\partial l_{exp}(H|\Phi)} {\partial h(x)} = -e^{-H(x)}P(f(x) = 1 | x) + e^{H(x)}P(f(x) = -1 | x)$$
这里假设$y_i \in \{-1, 1\}$, $f(x) \in \{ 1, -1 \}$
>$H_t(x)=\alpha_t h_t(x)$, $H_t(x)$应使损失函数$l_{exp}(H_t(x)|x)$最小, $h_t(x)$表示由$\Phi_t$分布获得
>$$ l_{exp}(\alpha_th_t(x)|\Phi_t) = E_{x \sim D_t}[e^{-\alpha_tf(x)h_t(x)}] = -e^{-\alpha_t}(1-\epsilon_t) + e^{\alpha_t}\epsilon_t $$
>$$ \frac{\partial l_{exp}(\alpha_t(1-\epsilon_t)}{\partial \alpha_t} = -e^{-\alpha_t}(1-\epsilon_t) +e^{\alpha_t}\epsilon_t$$
令上述导数为零得：
>$$ \alpha_t = \frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$$
AdaBoost算法在获得$H_{t-1}$后要对样本分布， 即样本的权值进行重新赋值，使下一轮的基学习器$h_t$能够纠正$H_{t-1}$的一些错误:
>$$\mathcal{l}_{exp} (H_{t-1}+h_t | \Phi) = \mathbb{E}_{x \sim \Phi}[e^{-f(x)(H_{t-1}(x)+h_t(x))}]=\mathbb{E}_{x \sim \Phi}[e^{-f(x)h_t(x)-f(x)H_{t-1}(x)}]$$
根据泰勒公式，将$e^{-f(x)h_t(x)}$展开， 其实就是求$e^{-x}$在零点处的泰勒展开式
>$$\mathbb{l}_{exp}(H_{t-1} + h_t | \Phi) \\ \simeq \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}(1-f(x)h_t(x) + \frac{f^2(x)h^2(x)}{2})] \\= \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}(1-f(x)h_t(x)+\frac{1}{2})]$$
所以理想的$h_t(x)$应为:
>$$h_t(x) = argmin\ \mathbb{l}_{exp}(H_{t-1}+h_t(x) | \Phi)\\ = argmin\ \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}(1-f(x)h_t(x)+\frac{1}{2})]\\ = argmax \ \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}f(x)h(x)\\  = argmax\ \mathbb{E}_{x \sim \Phi}[\frac{e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)]$$
因为$\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]$为一常数， 令:
>$$  \Phi_t(x) = \frac{\Phi(x)e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]} $$

$\Phi_t$表示第t个基分类器中样本的权重， $\Phi_{t+1}$可由如下公式获得：
>$$  \Phi_{t+1}(x) =  \frac{\Phi(x)e^{-f(x)H_{t}(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}\\ = \frac{\Phi(x)e^{-f(x)H_{t-1}(x)}e^{-f(x)\alpha_th_t(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}\\ = \Phi_t(x)e^{-f(x)\alpha_t(x)h_t(x)}\frac{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t}(x)}]}$$
