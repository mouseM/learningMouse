# 机器学习

## 集成学习

假设有$T$个基分类器， $h_i$表示第i个基分类器， $h_i(x)$表示第i个分类器对于输入$x$的预测结果， $f(x)$表示对于输入$x$的真实结果， 假设每个基分类器的错误率为$\epsilon$, 则有：
>$$  P(h_{i}(x) \neq f(x)) = \epsilon $$

假设超过半数基分类器预测正确则集成分类正确：
>$$ H(x) = sign(\sum_{i=n}^{T} h_i(x)) $$

假设每个分类器的误差相互独立， 根据$\mathcal{Hoeffding}$不等式， 有：
> $$ P(H(x) \neq f(x)) = \sum_{k=0}^{\lfloor T/2 \rfloor}\lgroup {T \atop k} \rgroup (1-\epsilon)^k \epsilon^(T - k)\le exp(- \frac{1}{2} (1-2\epsilon)^2) $$

所以随着基分类器数量的增加， 集成分类器的错误率将降低。

### Boosting

Boosting是一族可以将弱学习器提升为强学习器的算法， 最著名的代表是AdaBoost， 基学习器的线性组合为：
>$$  H(x) = \sum_{t = 1}^{T} \alpha_t h_t (x) $$
优化的损失函数为:
>$$  \mathcal{l}_{exp}(H | \Phi) = \mathbb{E}_{x \sim \Phi}[e^{-f(x)H(x)}] $$
>$$ \frac{\partial l_{exp}(H|\Phi)} {\partial h(x)} = -e^{-H(x)}P(f(x) = 1 | x) + e^{H(x)}P(f(x) = -1 | x)$$
这里假设$y_i \in \{-1, 1\}$, $f(x) \in \{ 1, -1 \}$
>$H_t(x)=\alpha_t h_t(x)$, $H_t(x)$应使损失函数$l_{exp}(H_t(x)|x)$最小, $h_t(x)$表示由$\Phi_t$分布获得
>$$ l_{exp}(\alpha_th_t(x)|\Phi_t) = E_{x \sim D_t}[e^{-\alpha_tf(x)h_t(x)}] = -e^{-\alpha_t}(1-\epsilon_t) + e^{\alpha_t}\epsilon_t $$
>$$ \frac{\partial l_{exp}(\alpha_t(1-\epsilon_t)}{\partial \alpha_t} = -e^{-\alpha_t}(1-\epsilon_t) +e^{\alpha_t}\epsilon_t$$
令上述导数为零得：
>$$ \alpha_t = \frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$$
AdaBoost算法在获得$H_{t-1}$后要对样本分布， 即样本的权值进行重新赋值，使下一轮的基学习器$h_t$能够纠正$H_{t-1}$的一些错误:
>$$\mathcal{l}_{exp} (H_{t-1}+h_t | \Phi) = \mathbb{E}_{x \sim \Phi}[e^{-f(x)(H_{t-1}(x)+h_t(x))}]=\mathbb{E}_{x \sim \Phi}[e^{-f(x)h_t(x)-f(x)H_{t-1}(x)}]$$
根据泰勒公式，将$e^{-f(x)h_t(x)}$展开， 其实就是求$e^{-x}$在零点处的泰勒展开式
>$$\mathbb{l}_{exp}(H_{t-1} + h_t | \Phi) \\ \simeq \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}(1-f(x)h_t(x) + \frac{f^2(x)h^2(x)}{2})] \\= \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}(1-f(x)h_t(x)+\frac{1}{2})]$$
所以理想的$h_t(x)$应为:
>$$h_t(x) = argmin\ \mathbb{l}_{exp}(H_{t-1}+h_t(x) | \Phi)\\ = argmin\ \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}(1-f(x)h_t(x)+\frac{1}{2})]\\ = argmax \ \mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}f(x)h(x)\\  = argmax\ \mathbb{E}_{x \sim \Phi}[\frac{e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)]$$
因为$\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]$为一常数， 令:
>$$  \Phi_t(x) = \frac{\Phi(x)e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]} $$

$\Phi_t$表示第t个基分类器中样本的权重， $\Phi_{t+1}$可由如下公式获得：
>$$  \Phi_{t+1}(x) =  \frac{\Phi(x)e^{-f(x)H_{t}(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}\\ = \frac{\Phi(x)e^{-f(x)H_{t-1}(x)}e^{-f(x)\alpha_th_t(x)}}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}\\ = \Phi_t(x)e^{-f(x)\alpha_t(x)h_t(x)}\frac{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t-1}(x)}]}{\mathbb{E}_{x \sim \Phi}[e^{-f(x)H_{t}(x)}]}$$

## MDS 多维缩放

假设$m$个样本在原始空间中的距离矩阵为$D \in \mathbb{R}^{m \times m}$, 其中第$i$行， 第$j$列的元素$dist_{ij}$为样本$x_i$到$x_j$的距离。我们的目标是获得样本在$d'$维空间的表示$Z \in \mathbb{R}^{d' \times m}$, $d' \le d$, 且任意两个样本在$d'$空间中的欧式距离等于原始空间中的距离， 即||$z_i - z_j$|| = $dist_{ij}$。
令$B = Z^TZ \in \mathbb{R}^{m \times m}$, 其中$B$为降维后的内积矩阵， $b_{ij} = z_i^Tz_j$, 有：
>$$ dist_{ij}^2 = ||z_i||^2 + ||z_j||^2 - 2z_i^Tz_j \\
=b_{ii} + b_{jj} - 2b_{ij} $$
令降维后的样本$Z$中心化, 即$\sum_{i=1}^m z_i = 0$, 有$\sum_{i=1}^mb_{ij} = \sum_{j=1}^mb_{ij} = 0$, 易知：
>$$ \sum_{i=1}^{m}dist_{ij}^2 = tr(B) + mb_{jj} \\
    \sum_{j=1}^{m}dist_{ij}^2 = tr(B) + mb_{ii} \\
    \sum_{i = 1}^{m} \sum_{j = 1}^{m}dist_{ij}^2 = 2mtr(B)$$
其中$tr(B) = \sum_{i = 1}^{m}||z_i||^2$, 令：
>$$ dist_{i\bullet}^2 = \frac{1}{m} \sum_{j=1}^{m}dist_{ij}^2
\\ dist_{\bullet j}^2 = \frac{1}{m}\sum_{i=1}^mdist_{ij}^2
\\ dist_{\bullet \bullet}^2 = \frac{1}{m^2}\sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2$$
所以有：
>$$ b_{ij} = \frac{1}{2}[dist_{\bullet j}^2 + dist_{i \bullet}^2 - dist_{\bullet \bullet}^2 - dist_{ij}^2] $$

对矩阵$B$做特征值分解， $B = V \Lambda V^T$， 其中$\Lambda = diag(\lambda_1, \lambda_2 ... \lambda_d)$为特征值构成的对角矩阵， $\lambda_1 \ge \lambda_2 \ge...\ge \lambda_d$,V为特征向量矩阵， 假设其中有$d^*$个非零特征值， 它们构成对角矩阵$\Lambda_* = diag(\lambda_1, \lambda_2, ... , \lambda_{d^*})$, 令$V_*$为对应的特征向量矩阵， 则$Z$可表示为：$Z = \Lambda_*^{\frac{1}{2}}V_*^T$。 (其实就是对$B$开方)
