# 机器学习

## 集成学习

假设有$T$个基分类器， $h_i$表示第i个基分类器， $h_i(x)$表示第i个分类器对于输入$x$的预测结果， $f(x)$表示对于输入$x$的真实结果， 假设每个基分类器的错误率为$\epsilon$, 则有：
>$$  P(h_{i}(x) \neq f(x)) = \epsilon $$

假设超过半数基分类器预测正确则集成分类正确：
>$$ H(x) = sign(\sum_{i=n}^{T} h_i(x)) $$

假设每个分类器的误差相互独立， 根据$\mathcal{Hoeffding}$不等式， 有：
> $$ P(H(x) \neq f(x)) = \sum_{k=0}^{\lfloor T/2 \rfloor}\lgroup {T \atop k} \rgroup (1-\epsilon)^k \epsilon^(T - k)\le exp(- \frac{1}{2} (1-2\epsilon)^2) $$

所以随着基分类器数量的增加， 集成分类器的错误率将降低。

### Boosting

Boosting是一族可以将弱学习器提升为强学习器的算法， 最著名的代表是AdaBoost， 基学习器的线性组合为：
>$$  H(x) = \sum_{t = 1}^{T} \alpha_t h_t (x) $$
优化的损失函数为:
>$$  \mathcal{l}_{exp}(H | \Phi) = \mathbb{E}_{x \sim \Phi}[e^{-f(x)H(x)}] $$
>$$ \frac{\partial l_{exp}(H|\Phi)} {\partial h(x)} = -e^{-H(x)}P(f(x) = 1 | x) + e^{H(x)}P(f(x) = -1 | x)$$
这里假设$y_i \in \{-1, 1\}$, $f(x) \in \{ 1, -1 \}$
>$H_t(x)=\alpha_t h_t(x)$, $H_t(x)$应使损失函数$l_{exp}(H_t(x)|x)$最小, $h_t(x)$表示由$\Phi_t$分布获得
>$$ l_{exp}(\alpha_th_t(x)|D_t) = E_{x \sim D_t}[e^{-\alpha_tf(x)h_t(x)}] = -e^{-\alpha_t}(1-\epsilon_t) + e^{\alpha_t}\epsilon_t $$
>$$ \frac{\partial l_{exp}(\alpha_t(1-\epsilon_t)}{\partial \alpha_t} = -e^{-\alpha_t}(1-\epsilon_t) +e^{\alpha_t}\epsilon_t$$
令上述导数为零得：
>$$ \alpha_t = \frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$$