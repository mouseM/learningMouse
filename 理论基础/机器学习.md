# 机器学习

## 集成学习

假设有$T$个基分类器， $h_i$表示第i个基分类器， $h_i(x)$表示第i个分类器对于输入$x$的预测结果， $f(x)$表示对于输入$x$的真实结果， 假设每个基分类器的错误率为$\epsilon$, 则有：
>$$  P(h_{i}(x) \neq f(x)) = \epsilon $$

假设超过半数基分类器预测正确则集成分类正确：
>$$ H(x) = sign(\sum_{i=n}^{T} h_i(x)) $$

假设每个分类器的误差相互独立， 根据$\mathcal{Hoeffding}$不等式， 有：
> $$ P(H(x) \neq f(x)) = \sum_{k=0}^{\lfloor T/2 \rfloor}\lgroup {T \atop k} \rgroup (1-\epsilon)^k \epsilon^(T - k)\le exp(- \frac{1}{2} (1-2\epsilon)^2) $$

所以随着基分类器数量的增加， 集成分类器的错误率将降低。

### Boosting

Boosting是一族可以将弱学习器提升为强学习器的算法， 最著名的代表是AdaBoost， 基学习器的线性组合为：
>$$  H(x) = \sum_{t = 1}^{T} \alpha_t h_t (x) $$
优化的损失函数为:
>$$  \mathcal{l}_{exp}(H | D) = \mathbb{E}_{x \sim D}[e^{-f(x)H(x)}] $$